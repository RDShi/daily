{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 特征的获取\n",
    "## 1.1 数据的获取\n",
    "获取数据时需要考虑和解决下面几个问题\n",
    "- 为了达到目标可能需要哪些数据，需要靠经验和对业务的理解来确定范围\n",
    "- 获取的难易程度\n",
    "- 获取到的数据的覆盖率（跨度是否与实际情况场景相符）和准确率（是否与真情情况相符）\n",
    "- 获取到的数据如何存储和调用，用sklearn时可以用pickle或者json，tensoflow时可以用tfrecord\n",
    "- 异常样本的清洗：简单的方法是3$\\sigma$法，超过均值3倍标准差的数据舍去（因为正态分布在均值3$\\sigma$范围内占总体的绝大部分）\n",
    "- 数据不平衡时的应对策略：1.数据量少的上采样；2.数据量多的分成若干份，训练多个模型bagging成一个模型\n",
    "\n",
    "## 1.2 特征的获取\n",
    "也就是把获得的数据变成特征，是个抽象的过程。常用的方法有：\n",
    "- 与平均值的距离：高于/低于平均值多少\n",
    "- 分位线：处于多少分位线处\n",
    "- 比例：处于不同状态的比例，例如男女比例，有点像归一化\n",
    "- 规则：符合某状态就把设置的特征设为1，否则为0，\n",
    "- 组合特征：形如既是A又是B的这种特征，可以认为设计，也可以用GBDT产生训练产生的路径作为组合特征\n",
    "\n",
    "选择特征可以减少冗余，去掉、减少或者组合掉不同特征之间过强的相关性（冗余会影响计算性能和过拟合）；\n",
    "也可以去掉和噪声一样无关的特征（噪声会对结果产生不利的影响）。\n",
    "\n",
    "P.S. 和降维有微妙的不同。降维是对已经选好的特征进行组合，而特征选择是剔除无关的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "feats_raw = iris.data\n",
    "labs_raw = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#dir(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 无量纲化\n",
    "保证不同特征本身数量上是平权的，以免因为取得单位不同带来的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = preprocessing.StandardScaler().fit_transform(feats_raw) #标准化，针对每一列，进行平移和防缩使得每一列均值和方差满足标准正态分布\n",
    "feats = preprocessing.MinMaxScaler().fit_transform(feats_raw)#区间防缩，针对每一列，进行平移和防缩使得每一列分布在0到1之间\n",
    "feats = preprocessing.Normalizer().fit_transform(feats_raw)#归一化，针对每一行，进行平移和防缩使得每一行均值和房车满足标准正态分布\n",
    "#or\n",
    "coder = preprocessing.StandardScaler().fit(feats_raw)\n",
    "feats = coder.transform(feats_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 离散化\n",
    "设定阈值当大于摸个值时为1，小于为0。可以先进行聚类然后进行，有点像哑编码，也可以先进行业务分析，把分为数作为阈值，分型多值化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = preprocessing.Binarizer(threshold=[3,4,3,2]).fit_transform(feats_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 One-Hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 5 7 3]\n"
     ]
    }
   ],
   "source": [
    "feats = ohcoder.transform(feats_raw).toarray()\n",
    "# or\n",
    "ohcoder = preprocessing.OneHotEncoder()\n",
    "ohcoder.fit(feats_raw)\n",
    "print(ohcoder.n_values_) # 每个特征对应的最大位数\n",
    "feats = ohcoder.transform(feats_raw).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 缺失值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack, array, nan\n",
    "#缺失值计算，返回值为计算缺失值后的数据\n",
    "#参数missing_value为缺失值的表示形式，默认为NaN\n",
    "#参数strategy为缺失值填充方式，默认为mean（均值），也可以是中位数，出现最多的值\n",
    "#实际中如果是离散的数据可以把缺失设置为特征的另外一个值，或者取最多的值\n",
    "#实际中也可以让缺失值随机以该位置的特征的分布的概率取值，或者把确实位置的值当作是label用其他特征预测进行填充\n",
    "coder = preprocessing.Imputer()\n",
    "coder.fit(vstack((array([nan, nan, nan, nan]), feats_raw)))\n",
    "feats = coder.transform(feats_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 数据变换\n",
    "把数据进行核函数变换到高维空间，可以是多项式、指数、对数函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多项式转换，参数degree为度，默认值为2\n",
    "coder = preprocessing.PolynomialFeatures()\n",
    "feats = coder.fit_transform(feats_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 特征选择\n",
    "当数据预处理完成后，需要选择有意义的特征输入机器学习的算法和模型进行训练。\n",
    "一般需要从两个方面考虑来选择特征：\n",
    "- 特征是否有\n",
    "- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "# dir(feature_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Filter\n",
    "按照差异性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n",
    "\n",
    "差异性：如果一个特征没差异，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，那么这个特征对于样本的区分并没有什么用。\n",
    "\n",
    "相关性：例如皮尔逊相关，互信息，距离相关度或者一些非线性相关系数。\n",
    "\n",
    "优点：速度快；缺点：没考虑特征之间的关联作用，有可能去掉有用的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "feats = feature_selection.VarianceThreshold(threshold=3).fit_transform(feats_raw)# 方差选择法,参数threshold为方差的阈值\n",
    "feats = feature_selection.SelectKBest(feature_selection.chi2, k=2).fit_transform(feats_raw, labs) # 保留相关性最大的k个特征\n",
    "# 第一个参数是计算相关性的函数，feature_selection.chi2是卡方检验，也可以自定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Wrapper\n",
    "根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\n",
    "可以看作是一个特征子集搜索问题。其实是一个NP-Hard问题，一般用一些启发式的方法。\n",
    "\n",
    "优点：科学准确；缺点：速度慢。\n",
    "\n",
    "P.S. sklearn.feature_selection中有递归特征消除法。\n",
    "方法是用某个模型跑出结果，剔除系数小的一部分特征，直到目标函数（准确率 or auc）有较大下滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "feats = feature_selection.RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(feats_raw, labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded\n",
    "先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。\n",
    "例如L正则化（L1或L2）、树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n",
      "d:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "feats = feature_selection.SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(feats_raw, labs)\n",
    "feats = feature_selection.SelectFromModel(GradientBoostingClassifier()).fit_transform(feats_raw, labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 降维\n",
    "PCA、LDA、FDA、tSNE etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "feats = PCA(n_components=2).fit_transform(feats_raw)\n",
    "from sklearn.lda import LDA\n",
    "feats = LDA(n_components=2).fit_transform(feats_raw, labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tips:\n",
    "可以对流程进行并行化（sklearn.pipeline.FeatureUnion）、流水线（sklearn.pipeline.Pipeline）、自动调参（sklearn.model_selection.GridSearchCV）、持久化（externals.joblib）等处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:\n",
    "\n",
    "- 1.[使用sklearn进行数据挖掘](http://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "- 2.[使用sklearn做单机特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
